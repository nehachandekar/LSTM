{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTaapFGZn_ob",
        "outputId": "26e76609-e8bf-42c3-bf5b-641769c039b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HRYtvSM0nz3X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcymFQObnz3Z"
      },
      "source": [
        "<hr>\n",
        "<i>Preview dataset</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsC4G3ACnz3g",
        "outputId": "c472bb46-07a4-40ab-c92a-73060b687289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  review sentiment\n",
            "0      One of the other reviewers has mentioned that ...  positive\n",
            "1      A wonderful little production. <br /><br />The...  positive\n",
            "2      I thought this was a wonderful way to spend ti...  positive\n",
            "3      Basically there's a family where a little boy ...  negative\n",
            "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "...                                                  ...       ...\n",
            "49995  I thought this movie did a down right good job...  positive\n",
            "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49997  I am a Catholic taught in parochial elementary...  negative\n",
            "49998  I'm going to have to disagree with the previou...  negative\n",
            "49999  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[50000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/LSTM/IMDB Dataset.csv')\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yh81BcWnz3i"
      },
      "source": [
        "\n",
        "<i>Declaring the english stop words</i>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cw6ZUWcRocRU",
        "outputId": "d0b35ea7-1263-446f-8660-612ed25f5cfc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WupulV6Pnz3i"
      },
      "outputs": [],
      "source": [
        "english_stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJ1HLpMnz3o"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Load and Clean Dataset\n",
        "\n",
        "\n",
        "### Encode Sentiments\n",
        "In the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5PrKdllnz3o",
        "outputId": "16833099-c497-4dbd-ddf3-81bfd292bad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews\n",
            "0        [one, reviewers, mentioned, watching, oz, epis...\n",
            "1        [a, wonderful, little, production, the, filmin...\n",
            "2        [i, thought, wonderful, way, spend, time, hot,...\n",
            "3        [basically, family, little, boy, jake, thinks,...\n",
            "4        [petter, mattei, love, time, money, visually, ...\n",
            "                               ...                        \n",
            "49995    [i, thought, movie, right, good, job, it, crea...\n",
            "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
            "49997    [i, catholic, taught, parochial, elementary, s...\n",
            "49998    [i, going, disagree, previous, comment, side, ...\n",
            "49999    [no, one, expects, star, trek, movies, high, a...\n",
            "Name: review, Length: 50000, dtype: object \n",
            "\n",
            "Sentiment\n",
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        0\n",
            "4        1\n",
            "        ..\n",
            "49995    1\n",
            "49996    0\n",
            "49997    0\n",
            "49998    0\n",
            "49999    0\n",
            "Name: sentiment, Length: 50000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def load_dataset():\n",
        "    df = pd.read_csv('/content/drive/MyDrive/LSTM/IMDB Dataset.csv')\n",
        "    x_data = df['review']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n",
        "\n",
        "print('Reviews')\n",
        "print(x_data, '\\n')\n",
        "print('Sentiment')\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddtcq9PQnz3p"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Split Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHpoDwO-nz3p",
        "outputId": "01890850-10a8-4796-8ec1-be231602a2e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set\n",
            "3099     [crossfire, remembered, much, fact, three, sta...\n",
            "37853    [this, story, long, awkward, love, the, daily,...\n",
            "16752    [i, vague, memories, movie, funny, having, see...\n",
            "2840     [star, rating, saturday, night, friday, night,...\n",
            "11770    [this, film, exactly, title, describes, attemp...\n",
            "                               ...                        \n",
            "39388    [i, understand, everyone, hates, movie, aside,...\n",
            "4804     [escanaba, da, moonlight, first, showcasing, j...\n",
            "32250    [loved, story, guy, tries, get, girl, back, do...\n",
            "17967    [the, pre, release, version, baby, face, would...\n",
            "17817    [the, scottish, artist, andy, goldsworthy, fas...\n",
            "Name: review, Length: 40000, dtype: object \n",
            "\n",
            "36857    [you, expect, movie, like, good, it, budget, u...\n",
            "45187    [as, others, mentioned, movie, similar, the, f...\n",
            "39528    [this, film, late, night, i, saw, it, interest...\n",
            "3108     [i, always, wanted, david, duchovney, go, movi...\n",
            "12744    [this, horrible, movie, all, three, stories, b...\n",
            "                               ...                        \n",
            "7982     [i, envy, barry, levinson, rachel, weisz, ben,...\n",
            "30385    [fairly, interesting, exploitation, flick, bla...\n",
            "7486     [the, first, murder, scene, one, best, murders...\n",
            "26072    [don, know, film, version, jeff, saw, entire, ...\n",
            "42369    [i, know, rather, unfair, comment, movie, with...\n",
            "Name: review, Length: 10000, dtype: object \n",
            "\n",
            "Test Set\n",
            "3099     1\n",
            "37853    1\n",
            "16752    0\n",
            "2840     0\n",
            "11770    1\n",
            "        ..\n",
            "39388    1\n",
            "4804     0\n",
            "32250    1\n",
            "17967    1\n",
            "17817    1\n",
            "Name: sentiment, Length: 40000, dtype: int64 \n",
            "\n",
            "36857    0\n",
            "45187    0\n",
            "39528    0\n",
            "3108     1\n",
            "12744    0\n",
            "        ..\n",
            "7982     0\n",
            "30385    0\n",
            "7486     1\n",
            "26072    1\n",
            "42369    0\n",
            "Name: sentiment, Length: 10000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
        "\n",
        "print('Train Set')\n",
        "print(x_train, '\\n')\n",
        "print(x_test, '\\n')\n",
        "print('Test Set')\n",
        "print(y_train, '\\n')\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yPntnSOnz3v"
      },
      "source": [
        "<hr>\n",
        "<i>Function for getting the maximum review length, by calculating the mean of all the reviews length (using <b>numpy.mean</b>)</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_Xybh2vYnz3v"
      },
      "outputs": [],
      "source": [
        "def get_max_length():\n",
        "    review_length = []\n",
        "    for review in x_train:\n",
        "        review_length.append(len(review))\n",
        "\n",
        "    return int(np.ceil(np.mean(review_length)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdLNRvk_nz3v"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Tokenize and Pad/Truncate Reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHspa7renz31",
        "outputId": "a16573a0-d538-4499-fcd5-cc6150c68c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded X Train\n",
            " [[8743 1923   17 ... 1592  183  275]\n",
            " [   8   13  100 ...   11  954   73]\n",
            " [   1 3475 1742 ...    0    0    0]\n",
            " ...\n",
            " [ 346   13  116 ...    0    0    0]\n",
            " [   2 1725  656 ... 4681 5319 4616]\n",
            " [   2 4450 1621 ...   16 3183 3789]] \n",
            "\n",
            "Encoded X Test\n",
            " [[ 103  433    3 ...    0    0    0]\n",
            " [ 108  304  959 ...   52 5189 4541]\n",
            " [   8    4  430 ...    0    0    0]\n",
            " ...\n",
            " [   2   23  482 ...    0    0    0]\n",
            " [ 333   47    4 ...    0    0    0]\n",
            " [   1   47  152 ...    0    0    0]] \n",
            "\n",
            "Maximum review length:  130\n"
          ]
        }
      ],
      "source": [
        "# ENCODE REVIEW\n",
        "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
        "token.fit_on_texts(x_train)\n",
        "x_train = token.texts_to_sequences(x_train)\n",
        "x_test = token.texts_to_sequences(x_test)\n",
        "\n",
        "max_length = get_max_length()\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
        "\n",
        "print('Encoded X Train\\n', x_train, '\\n')\n",
        "print('Encoded X Test\\n', x_test, '\\n')\n",
        "print('Maximum review length: ', max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I33i_uChnz32"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Build Architecture/Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPtT_Moanz32",
        "outputId": "84112dd1-4dad-4152-8605-eea40154610c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 130, 32)           2953664   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                24832     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2978561 (11.36 MB)\n",
            "Trainable params: 2978561 (11.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# ARCHITECTURE\n",
        "EMBED_DIM = 32\n",
        "LSTM_OUT = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
        "model.add(LSTM(LSTM_OUT))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wobbHQQSnz38"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MW1mkr3onz38"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'models/LSTM.h5',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vukk-e25nz38",
        "outputId": "25a6cf91-ed7f-41c9-bfe2-965544faea52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.7415\n",
            "Epoch 1: accuracy improved from -inf to 0.74145, saving model to models/LSTM.h5\n",
            "313/313 [==============================] - 47s 140ms/step - loss: 0.4740 - accuracy: 0.7415\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9233\n",
            "Epoch 2: accuracy improved from 0.74145 to 0.92325, saving model to models/LSTM.h5\n",
            "313/313 [==============================] - 27s 85ms/step - loss: 0.2160 - accuracy: 0.9233\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9604\n",
            "Epoch 3: accuracy improved from 0.92325 to 0.96038, saving model to models/LSTM.h5\n",
            "313/313 [==============================] - 16s 49ms/step - loss: 0.1282 - accuracy: 0.9604\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9794\n",
            "Epoch 4: accuracy improved from 0.96038 to 0.97943, saving model to models/LSTM.h5\n",
            "313/313 [==============================] - 14s 43ms/step - loss: 0.0756 - accuracy: 0.9794\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9856\n",
            "Epoch 5: accuracy improved from 0.97943 to 0.98563, saving model to models/LSTM.h5\n",
            "313/313 [==============================] - 10s 33ms/step - loss: 0.0528 - accuracy: 0.9856\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ad645bdf070>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7BWBl2fnz39"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ],
      "metadata": {
        "id": "sq6A9IwErk_D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXgZRgX4rsrH",
        "outputId": "d4e6eef6-8769-4b7c-896d-f54453682f13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 130, 32)           2953664   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                24832     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2978561 (11.36 MB)\n",
            "Trainable params: 2978561 (11.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test, batch_size = 128).argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qyvRwIhrxEa",
        "outputId": "35d4e31a-ed00-4bce-8b5c-ef6d41850941"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 1s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVZRl7QInz4J"
      },
      "source": [
        "---\n",
        "\n",
        "### Load Saved Model\n",
        "\n",
        "Load saved model and use it to predict a movie review statement's sentiment (positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xzkOeilinz4K"
      },
      "outputs": [],
      "source": [
        "loaded_model = load_model('models/LSTM.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLtTZZ-Ynz4K"
      },
      "source": [
        "Receives a review as an input to be predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9qOzsL3nz4P",
        "outputId": "763c4a28-0105-4e2e-8a33-72a038930da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie Review:  \"Bahubali\" fails to live up to its hype, offering little beyond flashy visuals and exaggerated action sequences. The film's plot is predictable and riddled with clichÃ©s, relying heavily on tired tropes of revenge and royalty. Character development is lacking, with one-dimensional protagonists and antagonists that fail to evoke empathy or interest. The dialogue is often cringe-worthy, filled with melodramatic speeches and wooden delivery. Despite its epic scale, the film struggles to maintain momentum, bogged down by unnecessary subplots and prolonged fight scenes. The female characters are particularly disappointing, reduced to mere props in a predominantly male-centric narrative. Overall, \"Bahubali\" is a superficial spectacle that prioritizes style over substance, ultimately leaving audiences unsatisfied and underwhelmed.\n"
          ]
        }
      ],
      "source": [
        "review = str(input('Movie Review: '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsgW1klqnz4Q"
      },
      "source": [
        "The input must be pre processed before it is passed to the model to be predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R12UPMLnz4Q",
        "outputId": "c9c7deba-26e8-425d-f3f6-8154c9b4da2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned:   Bahubali fails to live up to its hype offering little beyond flashy visuals and exaggerated action sequences The films plot is predictable and riddled with clichs relying heavily on tired tropes of revenge and royalty Character development is lacking with onedimensional protagonists and antagonists that fail to evoke empathy or interest The dialogue is often cringeworthy filled with melodramatic speeches and wooden delivery Despite its epic scale the film struggles to maintain momentum bogged down by unnecessary subplots and prolonged fight scenes The female characters are particularly disappointing reduced to mere props in a predominantly malecentric narrative Overall Bahubali is a superficial spectacle that prioritizes style over substance ultimately leaving audiences unsatisfied and underwhelmed\n",
            "Filtered:  [' bahubali fails live hype offering little beyond flashy visuals exaggerated action sequences the films plot predictable riddled clichs relying heavily tired tropes revenge royalty character development lacking onedimensional protagonists antagonists fail evoke empathy interest the dialogue often cringeworthy filled melodramatic speeches wooden delivery despite epic scale film struggles maintain momentum bogged unnecessary subplots prolonged fight scenes the female characters particularly disappointing reduced mere props predominantly malecentric narrative overall bahubali superficial spectacle prioritizes style substance ultimately leaving audiences unsatisfied underwhelmed']\n"
          ]
        }
      ],
      "source": [
        "# Pre-process input\n",
        "regex = re.compile(r'[^a-zA-Z\\s]')\n",
        "review = regex.sub('', review)\n",
        "print('Cleaned: ', review)\n",
        "\n",
        "words = review.split(' ')\n",
        "filtered = [w for w in words if w not in english_stops]\n",
        "filtered = ' '.join(filtered)\n",
        "filtered = [filtered.lower()]\n",
        "\n",
        "print('Filtered: ', filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_IJaFUMnz4X",
        "outputId": "85746e64-a7f6-4098-fe78-a1e1dd1e2a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  905   323  3338  3854    48   576  6277  2026  3991   110   737     2\n",
            "     35    40   619 10722  8350  2517  1342 19216  1070 12681    34   898\n",
            "   1763  3132 15285  1838  8095  5524   521     2   319   310 18223   950\n",
            "   3351  5546  1506  2689   372  1442  2233     4  2920  4325  7396 13435\n",
            "   1705  4390  9216   456    60     2   548    29   478  1250  3992  2715\n",
            "   4297 15624  1297   350  3791  5809   309  2285  1088  1111  1120 12248\n",
            "  23102     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "tokenize_words = token.texts_to_sequences(filtered)\n",
        "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "print(tokenize_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4UFoBRanz4Y"
      },
      "source": [
        "This is the result of the prediction which shows the **confidence score** of the review statement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RrdMjZ9nz4Y",
        "outputId": "0462b8e4-cfc3-423e-8caf-f78e23a5d05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "[[0.00170819]]\n"
          ]
        }
      ],
      "source": [
        "result = loaded_model.predict(tokenize_words)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTZAwjponz4d"
      },
      "source": [
        "If the confidence score is close to 0, then the statement is **negative**. On the other hand, if the confidence score is close to 1, then the statement is **positive**.here consider threshold of **0.7** to determine which confidence score is positive and negative, so if it is equal or greater than 0.7, it is **positive** and if it is less than 0.7, it is **negative**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPmqRVfWnz4e",
        "outputId": "19caf47c-c1d4-4e8d-f90f-94ab9d400749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ],
      "source": [
        "if result >= 0.7:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7ESBcONHnz4f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}